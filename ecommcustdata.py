# -*- coding: utf-8 -*-
"""EcommCustData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13LKGZWPXW5cvbh_FFfnS3A1Be3cUU_Ix

## Business Understanding

Make a prediction for how much customer spent in year

## Data Understanding
---

* Email
* Address
* Avatar
* Time on App
* Time on Website
* Length of Membership
* Yearly Amount Spent

### Download dataset
"""

!pip install -q kaggle

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d iabdulw/ecommerce-customer-data

# from google.colab import drive
# drive.mount('/content/drive')

"""## Data Preparation"""

# import libraries

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("darkgrid")

from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score , mean_squared_error

# load dataset

df = pd.read_csv('/content/ecommerce-customer-data.zip')

"""### Fix Data Structure"""

# head a data

df.head()

# change columns name

df = df.rename(columns = {"\tEmail": "Email"})
df.columns

# data info

df.info()

# data describe

df.describe()

"""### Handle missing value"""

# Check missing value

df.isna().sum()

"""### Data distribution"""

# time on app

sns.displot(df["Time on App"], kde= True, color="Blue")

# time on website

sns.displot(df["Time on Website"], kde=True, color="Blue")

sns.pairplot(df)

sns.heatmap(df.corr(), annot=True)

"""### Feature Scaling"""

# # create scaler

scaler = StandardScaler()

X = df[['Time on App', 'Time on Website', 'Length of Membership']]
y = df['Yearly Amount Spent']

# # fit and transform data

X = scaler.fit_transform(X)

"""## Modeling"""

random_states = [0, 42, 123, 456, 789]

for random_state in random_states:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)

    # Create linear regression object and train model
    reg = LinearRegression()
    reg.fit(X_train, y_train)

    # Evaluate model on test data
    score = reg.score(X_test, y_test)

    model = LinearRegression()

    # set hyperparameter to tune

    param_grid = {'copy_X': [True, False], 
                  'fit_intercept': [True, False], 
                  'n_jobs': [-1, 1], 
                  'positive': [True, False]}
    
    # perform a grid search

    grid_search = GridSearchCV(model, param_grid=param_grid, scoring='r2')
    grid_search.fit(X_train, y_train)

    print('Best Hyperparameters: ', grid_search.best_params_)

    # use the best hyperparameters to train model

    lin_reg_best = LinearRegression(**grid_search.best_params_)
    lin_reg_best.fit(X_train, y_train)

    y_pred = lin_reg_best.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print('Random State : ', random_state)
    print('MSE : ', mse)
    print("R-Squared : ", r2)
    print("\n")

# split dataset

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# create model

model = LinearRegression()

# set hyperparameter to tune

param_grid = {'copy_X': [True, False], 
              'fit_intercept': [True, False], 
              'n_jobs': [-1, 1], 
              'positive': [True, False]}

# perform a grid search

grid_search = GridSearchCV(model, param_grid=param_grid, scoring='r2')
grid_search.fit(X_train, y_train)

# print best hyperparameters

print('Best Hyperparameters: ', grid_search.best_params_)

# use the best hyperparameters to train model

lin_reg_best = LinearRegression(**grid_search.best_params_)
lin_reg_best.fit(X_train, y_train)

# use the model to make predict on the testing set

y_pred = lin_reg_best.predict(X_test)

"""## Evaluation"""

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('MSE : ', mse)
print("R-Squared : ", r2)

pd.DataFrame(y_pred)